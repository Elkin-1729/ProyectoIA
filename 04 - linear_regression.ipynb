{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Descargando dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24760\\728407879.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install kaggle # Instalando Kaggle CLI'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0muploaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Cargar API KEY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "!pip install kaggle # Instalando Kaggle CLI\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()  # Cargar API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moviendo API KEY a la carpeta de Kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "!kaggle competitions download -c online-sales -f TrainingDataset.csv -p /content # Descargando contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo Entrega Final Proyecto IA**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importamos lobrerias necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De sklearn importamos el modelo LinearRegression, en el cual especificaremos dos métodos: fit intercept=False porque después de varias pruebas se pudo apreciar una optimización en el tiempo de ejecución con este parámetro en False, ademas se le indico que positive=True para que solo nos devolviera valores positivos, ya que son ventas de un producto y no debería de darnos negativo nunca.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "estimator = LinearRegression(fit_intercept=False, positive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abrimos los datos de trabajo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/Elkin-1729/ProyectoIA/main/data/datos_procesados.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos nuestra función de relación, que nos da información sobre el peso e importancia de las columnas, esto nos servirá para terminar de purgar nuestra base de datos y no incluir las columnas que tengan una relación inferior a 10%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisis_de_relacion(df, df1, df2, por):\n",
    "    corrmat = df.corr()\n",
    "    relacion = {}\n",
    "\n",
    "    for i in df1:\n",
    "        lista = []\n",
    "        for j in df2:\n",
    "            if corrmat[i][j] > por:\n",
    "                lista.append(j)\n",
    "        if lista:\n",
    "            relacion[i] = lista\n",
    "        return lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el evaluador RMSLE (root mean square logarithmic error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_r, y_p):\n",
    "    result = (sum((np.log(abs(y_p) + 1) - np.log(abs(y_r) + 1)) ** 2) / len(y_p)) ** 0.5\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función que va a partir la base de datos en dos, utilizaremos una parte de los datos para entrenar el modelo de regresión y la otra se utilizara para hacer una primera predicción, con la cual podremos evaluar que tan bien quedo el ajuste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fit_predict(estimator, X, y, train_pct):\n",
    "    def split_data(X, y, pct):\n",
    "        assert len(X) == len(y), \"X and y must have the same length\"\n",
    "        assert pct > 0 and pct < 1, \"pct must be in the (0,1) iterval\"\n",
    "\n",
    "        permutation = np.random.permutation(range(len(X)))\n",
    "        per = permutation\n",
    "        n1_elements = int(len(X) * pct)\n",
    "        permutation_partition_1 = permutation[:n1_elements]\n",
    "        permutation_partition_2 = permutation[n1_elements:]\n",
    "        X1 = X[permutation_partition_1]\n",
    "        X2 = X[permutation_partition_2]\n",
    "        y1 = y[permutation_partition_1]\n",
    "        y2 = y[permutation_partition_2]\n",
    "\n",
    "        return X1, X2, y1, y2, per\n",
    "\n",
    "    Xtr, Xts, ytr, yts, per = split_data(X, y, train_pct)\n",
    "\n",
    "    estimator.fit(Xtr, ytr)\n",
    "    preds_ts = estimator.predict(Xts)\n",
    "\n",
    "    return (\n",
    "        np.round(per),\n",
    "        estimator,\n",
    "        np.round(Xts),\n",
    "        np.round(yts),\n",
    "        accuracy(yts, preds_ts),\n",
    "        np.round(preds_ts),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección separaremos la base de datos original en diferentes grupos, X_p y y_p los utilizaremos para hacer una ultima predicción y comprobar la eficiencia del modelo, X y y se utilizaran para el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.iloc[:, :12]\n",
    "df2 = df.iloc[:, 12:]\n",
    "\n",
    "relacion = analisis_de_relacion(df, df1, df2, 0.1)\n",
    "\n",
    "X_o = np.array(df[relacion])\n",
    "y_o = np.array(df1)\n",
    "\n",
    "por = 0.2\n",
    "X_p = X_o[int(len(X_o) * por) :]\n",
    "y_p = np.array(y_o[int(len(y_o) * por) :])\n",
    "\n",
    "X = X_o[: int(len(X_o) * por)]\n",
    "y = y_o[: int(len(y_o) * por)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3000.,  3000.,  2000., ...,   500.,  2000.,   500.],\n",
       "       [35000., 11000.,  2000., ...,  2000.,  2000.,  2000.],\n",
       "       [10000.,  3000.,  3000., ...,  2000.,  2000.,  2000.],\n",
       "       ...,\n",
       "       [10000.,  2000.,   500., ...,   500.,  2000.,   500.],\n",
       "       [11000.,  3000.,  2000., ...,  2000.,   500.,   500.],\n",
       "       [18000.,  6000.,  2000., ...,  2000.,  2000.,  3000.]])"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección hacemos un ciclo, en el cual aplicamos el método creado anteriormente, repitiéndolo hasta encontrar un valor aceptable, este valor se tomo analizando las soluciones, se encontró que puede tener valores inferiores a 0.82, pero lo mejor para la optimización en tiempo es dejarlo en 0.83.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "\n",
    "\n",
    "while True:\n",
    "    prediccion = split_fit_predict(estimator, X, y, 0.2)\n",
    "\n",
    "    c += 1\n",
    "\n",
    "    if np.mean(prediccion[-2]) < 0.83:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se extrae el estimator entrenado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_entrenado = prediccion[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos predicciones tanto a la base de datos original como a la que separamos y medimos su error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.round(estimator_entrenado.predict(X_p))\n",
    "\n",
    "error = accuracy(y_p, pred)\n",
    "\n",
    "predT = np.round(estimator_entrenado.predict(X_o))\n",
    "\n",
    "errorT = accuracy(y_o, predT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "como podemos observar el error es muy bajo en algunas predicciones llegando en ocasiones a valores inferiores a 0.7 para algunas columnas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.95027117, 1.09752583, 1.00365922, 0.99947819, 0.92818242,\n",
       "        0.81440812, 0.74950759, 0.7021851 , 0.85772613, 0.80585501,\n",
       "        0.62590658, 0.59614803]),\n",
       " array([0.96968636, 1.11070876, 1.01558873, 1.02504867, 0.9549249 ,\n",
       "        0.82857594, 0.76759381, 0.71526858, 0.86591562, 0.80384771,\n",
       "        0.64064875, 0.60792029]))"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorT, error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
